@InProceedings{10.1007/978-3-642-34182-3_4,
author="van Beurden, Maurice H. P. H.
and Ijsselsteijn, Wijnand A.
and de Kort, Yvonne A. W.",
editor="Efthimiou, Eleni
and Kouroupetroglou, Georgios
and Fotinea, Stavroula-Evita",
title="User Experience of Gesture Based Interfaces: A Comparison with Traditional Interaction Methods on Pragmatic and Hedonic Qualities",
booktitle="Gesture and Sign Language in Human-Computer Interaction and Embodied Communication",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="36--47",
abstract="Studies into gestural interfaces -- and interfaces in general - typically focus on pragmatic or usability aspects (e.g., ease of use, learnability). Yet the merits of gesture-based interaction likely go beyond the purely pragmatic and impact a broader class of experiences, involving also qualities such as enjoyment, stimulation, and identification. The current study compared gesture-based interaction with device-based interaction, in terms of both their pragmatic and hedonic qualities. Two experiments were performed, one in a near-field context (mouse vs. gestures), and one in a far-field context (Wii vs. gestures). Results show that, whereas device-based interfaces generally scored higher on perceived performance, and the mouse scored higher on pragmatic quality, embodied interfaces (gesture-based interfaces, but also the Wii) scored higher in terms of hedonic quality and fun. A broader perspective on evaluating embodied interaction technologies can inform the design of such technologies and allow designers to tailor them to the appropriate application.",
isbn="978-3-642-34182-3"
}

@article{10.1145/274430.274436,
author = {Myers, Brad A.},
title = {A Brief History of Human-Computer Interaction Technology},
year = {1998},
issue_date = {March/April 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/274430.274436},
doi = {10.1145/274430.274436},
journal = {Interactions},
month = mar,
pages = {44–54},
numpages = {11}
}

@inproceedings{bhuiyan2009gesture,
  title={Gesture-controlled user interfaces, what have we done and what’s next},
  author={Bhuiyan, Moniruzzaman and Picking, Rich},
  booktitle={Proceedings of the fifth collaborative research symposium on security, e-learning, internet and networking (SEIN 2009), Darmstadt, Germany},
  pages={25--29},
  year={2009},
  organization={Citeseer}
}

@article{doi:10.1177/0018720818824253,
author = {Lisa Graichen and Matthias Graichen and Josef F. Krems},
title ={Evaluation of Gesture-Based In-Vehicle Interaction: User Experience and the Potential to Reduce Driver Distraction},
journal = {Human Factors},
volume = {61},
number = {5},
pages = {774-792},
year = {2019},
doi = {10.1177/0018720818824253},
    note ={PMID: 30694705},

URL = { 
        https://doi.org/10.1177/0018720818824253
    
},
eprint = { 
        https://doi.org/10.1177/0018720818824253
    
}
,
    abstract = { Objective:We observe the effects of in-vehicle system gesture-based interaction versus touch-based interaction on driver distraction and user experience.Background:Driver distraction is a major problem for traffic safety, as it is a contributing factor to a number of accidents. Visual distraction in particular has a highly negative impact on the driver. One possibility for reducing visual driver distraction is to use new forms of interaction in the vehicle, such as gesture-based interaction.Method:In this experiment, participants drove on a motorway or in a city scenario while using touch-based interaction or gesture-based interaction. Subjective data, such as acceptance and workload, and objective data, including glance behavior, were gathered.Results:As a result, participants rated their subjective impressions of safe driving as higher when using gesture-based interaction. More specifically, acceptance and attractiveness were higher, and workload was lower. The participants performed significantly fewer glances to the display and the glances were much shorter.Conclusion:Gestures are a positive alternative for in-vehicle interaction since effects on driver distraction are less significant when compared to touch-based interaction.Application:Potential application of this research includes interaction design of typical in-vehicle information and entertainment functions. }
}

@article{jbp:/content/journals/10.1075/pc.7.1.04gul,
   author = "Gullberg, Marianne and Holmqvist, Kenneth",
   title = "Keeping an eye on gestures: Visual perception of gestures in face-to-face communication", 
   journal= "Pragmatics \& Cognition",
   year = "1999",
   volume = "7",
   number = "1",
   pages = "35-63",
   doi = "https://doi.org/10.1075/pc.7.1.04gul",
   url = "https://www.jbe-platform.com/content/journals/10.1075/pc.7.1.04gul",
   publisher = "John Benjamins",
   issn = "0929-0907",
   type = "Journal Article",
   abstract = "Since listeners usually look at the speaker&apos;s face, gestural information has to be absorbed through peripheral visual perception. In the literature, it has been suggested that listeners look at gestures under certain circumstances: 1) when the articulation of the gesture is peripheral; 2) when the speech channel is insufficient for comprehension; and 3) when the speaker him- or herself indicates that the gesture is worthy of attention. The research here reported employs eye tracking techniques to study the perception of gestures in face-to-face interaction. The improved control over the listener&apos;s visual channel allows us to test the validity of the above claims. We present preliminary findings substantiating claims 1 and 3, and relate them to theoretical proposals in the literature and to the issue of how visual and cognitive attention are related.",
  }

@article{https://doi.org/10.1080/00207597508247319,
author = {Graham, Jean Ann and Argyle, Michael},
title = {A cross-cultural study of the communication of extra-verbal meaning by gestures(1)},
journal = {International Journal of Psychology},
volume = {10},
number = {1},
pages = {57-67},
doi = {https://doi.org/10.1080/00207597508247319},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/00207597508247319},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1080/00207597508247319},
abstract = {English and Italian encoders were asked to communicate two-dimensional shapes to decoders of their own culture, with and without the use of hand gestures, for materials of high and low verbal codability. The decoders drew what they thought the shapes were and these were rated by English and Italian judges, for similarity to the originals. Higher accuracy scores were obtained by both the English and the Italians, when gestures were allowed, for materials of both high and low codability; but the effect of using gestures was greater for materials of low codability. Improvement in performance when gestures were allowed was greater for the Italians than for the English for both levels of codability. An analysis of the recorded verbal utterances has shown that the detriment in communication accuracy with the elimination of gestures cannot be attributed to disruption of speech performance; rather, changes in speech content occur indicating an increased reliance on verbal means of conveying spatial information. Nevertheless, gestures convey this kind of semantic information more accurately and evidence is provided for the gestures of the Italians communicating this information more effectively than those of the English.},
year = {1975}
}

@article{OBERMEIER2012857,
title = "The benefit of gestures during communication: Evidence from hearing and hearing-impaired individuals",
journal = "Cortex",
volume = "48",
number = "7",
pages = "857 - 870",
year = "2012",
note = "Language and the Motor System",
issn = "0010-9452",
doi = "https://doi.org/10.1016/j.cortex.2011.02.007",
url = "http://www.sciencedirect.com/science/article/pii/S0010945211000323",
author = "Christian Obermeier and Thomas Dolk and Thomas C. Gunter",
keywords = "Communication, Gesture–speech integration, Hearing impaired, Babble noise, Asynchrony, N400",
abstract = "There is no doubt that gestures are communicative and can be integrated online with speech. Little is known, however, about the nature of this process, for example, its automaticity and how our own communicative abilities and also our environment influence the integration of gesture and speech. In two Event Related Potential (ERP) experiments, the effects of gestures during speech comprehension were explored. In both experiments, participants performed a shallow task thereby avoiding explicit gesture–speech integration. In the first experiment, participants with normal hearing viewed videos in which a gesturing actress uttered sentences which were either embedded in multi-speaker babble noise or not. The sentences contained a homonym which was disambiguated by the information in a gesture, which was presented asynchronous to speech (1000msec earlier). Downstream, the sentence contained a target word that was either related to the dominant or subordinate meaning of the homonym and was used to indicate the success of the disambiguation. Both the homonym and the target word position showed clear ERP evidence of gesture–speech integration and disambiguation only under babble noise. Thus, during noise, gestures were taken into account as an important communicative cue. In Experiment 2, the same asynchronous stimuli were presented to a group of hearing-impaired students and age-matched controls. Only the hearing-impaired individuals showed significant speech–gesture integration and successful disambiguation at the target word. The age-matched controls did not show any effect. Thus, individuals who chronically experience suboptimal communicative situations in daily life automatically take gestures into account. The data from both experiments indicate that gestures are beneficial in countering difficult communication conditions independent of whether the difficulties are due to external (babble noise) or internal (hearing impairment) factors."
}

@article{WU2007234,
title = "How iconic gestures enhance communication: An ERP study",
journal = "Brain and Language",
volume = "101",
number = "3",
pages = "234 - 245",
year = "2007",
note = "Gesture, Brain, and Language",
issn = "0093-934X",
doi = "https://doi.org/10.1016/j.bandl.2006.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0093934X0600438X",
author = "Ying Choon Wu and Seana Coulson",
keywords = "Gesture, N400, N300, Semantic integration, Language comprehension, Object recognition, Conceptual integration, Embodiment, ERP, Meaning, Simulation",
abstract = "EEG was recorded as adults watched short segments of spontaneous discourse in which the speaker’s gestures and utterances contained complementary information. Videos were followed by one of four types of picture probes: cross-modal related probes were congruent with both speech and gestures; speech-only related probes were congruent with information in the speech, but not the gesture; and two sorts of unrelated probes were created by pairing each related probe with a different discourse prime. Event-related potentials (ERPs) elicited by picture probes were measured within the time windows of the N300 (250–350ms post-stimulus) and N400 (350–550ms post-stimulus). Cross-modal related probes elicited smaller N300 and N400 than speech-only related ones, indicating that pictures were easier to interpret when they corresponded with gestures. N300 and N400 effects were not due to differences in the visual complexity of each probe type, since the same cross-modal and speech-only picture probes elicited N300 and N400 with similar amplitudes when they appeared as unrelated items. These findings extend previous research on gesture comprehension by revealing how iconic co-speech gestures modulate conceptualization, enabling listeners to better represent visuo-spatial aspects of the speaker’s meaning."
}

@article{https://doi.org/10.1111/1460-6984.12535,
author = {McWeeny, Sean and Norton, Elizabeth S.},
title = {Understanding event-related potentials (ERPs) in clinical and basic language and communication disorders research: a tutorial},
journal = {International Journal of Language \& Communication Disorders},
volume = {55},
number = {4},
pages = {445-457},
keywords = {event-related potential (ERP), neuroscience, mismatch negativity, N400},
doi = {https://doi.org/10.1111/1460-6984.12535},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1460-6984.12535},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1460-6984.12535},
abstract = {Abstract Background Event-related potentials (ERPs), which are electrophysiological neural responses time-locked to a stimulus, have become an increasingly common tool in language and communication disorders research. They can provide complementary evidence to behavioural measures as well as unique perspectives on communication disorders. ERPs have the distinct advantage of providing precise information about the timing of neural processes and can be used in cases where it is difficult to obtain responses from participants, such as infants or individuals who are minimally verbal. However, clinicians and clinician–scientists rarely receive training in how to interpret ERP research. Aims To provide information that allows readers to better understand, interpret and evaluate research using ERPs. We focus on research related to communication sciences and disorders and the information that is most relevant to interpreting research articles. Method We explain what ERPs are and how ERP data are collected, referencing key texts and primary research articles. Potential threats to validity, guidelines for interpreting data, and the pros and cons using of ERPs are discussed. Research in the area of paediatric language disorders is used as a model; common paradigms such as the semantic incongruity N400 and auditory mismatch negativity are used as tangible examples. With this foundation of understanding ERPs, the state of the field in terms of how ERPs are used and the ways they may inform the field are discussed. Main Contribution To date, no review has focused on ERPs as they relate to clinical or communication research. The main contribution of this review is that it provides practical information geared toward understanding ERP research. Conclusions ERPs offer insights into neural processes supporting communication and can both complement behaviour and provide information that behavioural measures cannot. We encourage readers to evaluate articles using ERPs critically, effectively pushing the field forward through increased understanding and rigor. What this paper adds ERPs have become more prevalent in research relevant to communication sciences and disorders. In order for clinicians to review and evaluate this research, an understanding of ERPs is needed. This review adds to the field by providing an accessible description of what ERPs are, a description of what ERP components are, and the most relevant commonly used components, as well as how ERP data are recorded and processed. With this foundational understanding of how ERPs work, guidelines for the interpretation of ERP data are given. Though few ERP studies currently have direct implications for clinical practice, we discuss several ways through which ERPs can impact clinical practice in future, by providing information that cannot be obtained by behaviour alone about the aetiology of disorders, and as potential biomarkers of disorder or treatment response.},
year = {2020}
}

@article{tsai2020design,
  title={Design of hand gesture recognition system for human-computer interaction},
  author={Tsai, Tsung-Han and Huang, Chih-Chi and Zhang, Kung-Long},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={9},
  pages={5989--6007},
  year={2020},
  publisher={Springer}
}

@article{shanthakumar2020design,
  title={Design and evaluation of a hand gesture recognition approach for real-time interactions},
  author={Shanthakumar, Vaidyanath Areyur and Peng, Chao and Hansberger, Jeffrey and Cao, Lizhou and Meacham, Sarah and Blakely, Victoria},
  journal={Multimedia Tools and Applications},
  pages={1--24},
  year={2020},
  publisher={Springer}
}

@article{yasen2019systematic,
  title={A systematic review on hand gesture recognition techniques, challenges and applications},
  author={Yasen, Mais and Jusoh, Shaidah},
  journal={PeerJ Computer Science},
  volume={5},
  pages={e218},
  year={2019},
  publisher={PeerJ Inc.}
}

@article{10.1145/1897816.1897838,
author = {Wachs, Juan Pablo and K\"{o}lsch, Mathias and Stern, Helman and Edan, Yael},
title = {Vision-Based Hand-Gesture Applications},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/1897816.1897838},
doi = {10.1145/1897816.1897838},
abstract = {Body posture and finger pointing are a natural modality for human-machine interaction, but first the system must know what it's seeing.},
journal = {Commun. ACM},
month = feb,
pages = {60–71},
numpages = {12}
}

@article{cheok2019review,
  title={A review of hand gesture and sign language recognition techniques},
  author={Cheok, Ming Jin and Omar, Zaid and Jaward, Mohamed Hisham},
  journal={International Journal of Machine Learning and Cybernetics},
  volume={10},
  number={1},
  pages={131--153},
  year={2019},
  publisher={Springer}
}

@Article{jimaging6080073,
AUTHOR = {Oudah, Munir and Al-Naji, Ali and Chahl, Javaan},
TITLE = {Hand Gesture Recognition Based on Computer Vision: A Review of Techniques},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {73},
URL = {https://www.mdpi.com/2313-433X/6/8/73},
ISSN = {2313-433X},
ABSTRACT = {Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human&ndash;computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications.},
DOI = {10.3390/jimaging6080073}
}

@article{shafer2019factors,
  title={Factors affecting enjoyment of virtual reality games: a comparison involving consumer-grade virtual reality technology},
  author={Shafer, Daniel M and Carbonara, Corey P and Korpi, Michael F},
  journal={Games for health journal},
  volume={8},
  number={1},
  pages={15--23},
  year={2019},
  publisher={Mary Ann Liebert, Inc., publishers 140 Huguenot Street, 3rd Floor New~…}
}

@inproceedings{shelstad2017gaming,
  title={Gaming on the rift: How virtual reality affects game user satisfaction},
  author={Shelstad, William J and Smith, Dustin C and Chaparro, Barbara S},
  booktitle={Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume={61},
  pages={2072--2076},
  year={2017},
  organization={SAGE Publications Sage CA: Los Angeles, CA}
}
